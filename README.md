# Sanskrit RAG System

A production-ready Retrieval-Augmented Generation (RAG) system for Sanskrit texts, designed for **CPU-only inference**. This system ingests Sanskrit moral stories, indexes them using a hybrid strategy (BM25 + Semantic Vectors), and generates context-aware answers using a local Large Language Model.

## ğŸš€ Unique Features

*   **Cross-Script Support**: Seamlessly handles **Devanagari** (à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤), **IAST** (saá¹ƒská¹›ta), and **Loose Roman** (sanskrit) inputs.
*   **Hybrid Retrieval**: Combines Lexical (BM25) precision with Semantic (embedding) understanding.
*   **CPU Optimized**: efficient inference using `llama.cpp` and `quantized` models.
*   **Citation Aware**: Every answer cites the source story title.

---

## ğŸ› ï¸ Installation

### Prerequisites
*   Python 3.10+
*   Allowed: 8GB+ RAM
*   OS: Windows, Linux, or MacOS

### Setup
1.  **Clone the repository**
    ```bash
    git clone <repository_url>
    cd RAG_Sanskrit_SarvagyaJain
    ```

2.  **Create Virtual Environment**
    ```bash
    python -m venv venv
    # Windows
    .\venv\Scripts\activate
    # Linux/Mac
    source venv/bin/activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Download Models**
    The system requires two models. Place them in the `models/` directory:
    
    *   **LLM**: [Qwen2.5-3B-Instruct-Q5_K_M.gguf](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF)
        *   Save to: `models/llm/Qwen2.5-3B-Instruct-Q5_K_M.gguf`
    *   **Embedding**: [intfloat/multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) 
        *   *Note: This is downloaded automatically by `sentence-transformers` on first run.*

---

## ğŸƒ Usage

The system exposes a unified CLI via `code/main.py`.

### 1. Indexing (First Time Setup)
Ingest and index the raw Sanskrit stories:
```bash
python code/main.py --mode index --data ./data/raw
```

### 2. Interactive Query Mode (Recommended)
Start the chatbot interface:
```bash
python code/main.py --interactive
```
*   *Type your question in English or Sanskrit (e.g., "Who was Shankhanada?" or "à¤¶à¤‚à¤–à¤¨à¤¾à¤¦à¤ƒ à¤•à¤ƒ à¤†à¤¸à¥€à¤¤à¥?")*

### 3. Quick Query
Run a single query from command line:
```bash
python code/main.py --mode query --query "à¤•à¤¾à¤²à¥€à¤¦à¤¾à¤¸à¤¸à¥à¤¯ à¤µà¤¿à¤·à¤¯à¥‡ à¤•à¤¿à¤®à¥ à¤µà¤°à¥à¤£à¤¿à¤¤à¤®à¥?"
```

---

## ğŸ“‚ Project Structure

```
RAG_Sanskrit_SarvagyaJain/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ main.py               # Main CLI Entry Point
â”‚   â”œâ”€â”€ config/               # System configuration (YAML)
â”‚   â”œâ”€â”€ src/                  # Source Code
â”‚   â”‚   â”œâ”€â”€ ingestion/        # Document loading & segmentation
â”‚   â”‚   â”œâ”€â”€ preprocessing/    # Script normalization (SLP1)
â”‚   â”‚   â”œâ”€â”€ indexing/         # BM25 & Vector Indexing
â”‚   â”‚   â”œâ”€â”€ retrieval/        # Hybrid Retrieval Logic
â”‚   â”‚   â””â”€â”€ generation/       # LLM Integration (Qwen)
â”‚   â””â”€â”€ scripts/              # Utility scripts (eval, debug)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Original stories.txt
â”‚   â””â”€â”€ processed/            # Indexed artifacts (FAISS, BM25)
â”œâ”€â”€ models/                   # Local GGUF models
â””â”€â”€ report/                   # Documentation
    â””â”€â”€ Technical_Report.md   # Detailed System Report
```

## ğŸ“„ Documentation

For a deep dive into the architecture, retrieval strategy, and performance metrics, please read the **[Technical Report](report/Technical_Report.md)**.

## âš–ï¸ License
MIT License